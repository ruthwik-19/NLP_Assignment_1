{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ruthw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ruthw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ruthw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function with stopword removal and lemmatization\n",
    "def preprocess_review(text, stop_words, lemmatizer):\n",
    "    \"\"\"Converts text to lowercase, removes punctuation, removes stopwords, lemmatizes, and tokenizes it.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Remove punctuation\n",
    "    tokens = text.split()  # Split by whitespace\n",
    "\n",
    "    # Remove stopwords and lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read corpus\n",
    "def read_corpus(filename, stop_words, lemmatizer):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    tokens = []\n",
    "    for line in lines:\n",
    "        tokens.extend(preprocess_review(line, stop_words, lemmatizer))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count n-grams\n",
    "def count_ngrams(tokens, n):\n",
    "    ngram_counts = defaultdict(int)\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = tuple(tokens[i:i + n])\n",
    "        ngram_counts[ngram] += 1\n",
    "    return ngram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate unsmoothed probabilities\n",
    "def calculate_probabilities(ngram_counts, unigram_counts=None):\n",
    "    probabilities = {}\n",
    "    if unigram_counts:\n",
    "        for ngram, count in ngram_counts.items():\n",
    "            probabilities[ngram] = count / unigram_counts[(ngram[0],)]\n",
    "    else:\n",
    "        total_unigrams = sum(ngram_counts.values())\n",
    "        probabilities = {unigram: count / total_unigrams for unigram, count in ngram_counts.items()}\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle unknown words using Byte Pair Encoding (BPE)\n",
    "def handle_unknown_words_bpe(tokens, min_freq=5):\n",
    "    word_freq = Counter(tokens)\n",
    "    vocab = {word for word, count in word_freq.items() if count >= min_freq}\n",
    "    tokens = [word if word in vocab else \"<UNK>\" for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Function to replace unknown words with nearest neighbor using Word2Vec\n",
    "def handle_unknown_words_nearest_neighbor(tokens, model):\n",
    "    known_vocab = set(model.wv.index_to_key)\n",
    "    updated_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in known_vocab:\n",
    "            updated_tokens.append(token)\n",
    "        else:\n",
    "            # Find the most similar word if token is unknown\n",
    "            similar_word = find_nearest_neighbor(token, known_vocab, model)\n",
    "            updated_tokens.append(similar_word if similar_word else \"<UNK>\")\n",
    "    return updated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find nearest neighbor using Word2Vec\n",
    "def find_nearest_neighbor(word, known_vocab, model):\n",
    "    try:\n",
    "        max_similarity = -1\n",
    "        similar_word = None\n",
    "        word_vector = model.wv[word]\n",
    "\n",
    "        for known_word in known_vocab:\n",
    "            similarity = cosine_similarity([word_vector], [model.wv[known_word]])[0][0]\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                similar_word = known_word\n",
    "\n",
    "        if similar_word is None:\n",
    "            return '<UNK>'\n",
    "        return similar_word\n",
    "    except KeyError:\n",
    "        # If the word is not in the model's vocabulary, return '<UNK>' or handle differently\n",
    "        return '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_laplace_smoothing(ngram_counts, unigram_counts, vocab_size, n):\n",
    "    probabilities = {}\n",
    "    for ngram in ngram_counts:\n",
    "        if n == 2:\n",
    "            probabilities[ngram] = (ngram_counts[ngram] + 1) / (unigram_counts[(ngram[0],)] + vocab_size)\n",
    "        elif n == 1:\n",
    "            total_unigrams = sum(unigram_counts.values())\n",
    "            probabilities[ngram] = (ngram_counts[ngram] + 1) / (total_unigrams + vocab_size)\n",
    "    return probabilities\n",
    "\n",
    "# Function to apply add-k smoothing\n",
    "def apply_add_k_smoothing(ngram_counts, unigram_counts, vocab_size, n, k=0.5):\n",
    "    probabilities = {}\n",
    "    for ngram in ngram_counts:\n",
    "        if n == 2:\n",
    "            probabilities[ngram] = (ngram_counts[ngram] + k) / (unigram_counts[(ngram[0],)] + k * vocab_size)\n",
    "        elif n == 1:\n",
    "            total_unigrams = sum(unigram_counts.values())\n",
    "            probabilities[ngram] = (ngram_counts[ngram] + k) / (total_unigrams + k * vocab_size)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate perplexity\n",
    "def calculate_perplexity(tokens, ngram_probs, n):\n",
    "    N = len(tokens) - n + 1\n",
    "    log_sum = 0\n",
    "    for i in range(N):\n",
    "        ngram = tuple(tokens[i:i + n])\n",
    "        if ngram in ngram_probs:\n",
    "            prob = ngram_probs[ngram]\n",
    "        else:\n",
    "            prob = 1e-8  # Assign a very small probability for unknown n-grams\n",
    "        log_sum += math.log(prob)\n",
    "    return math.exp(-log_sum / N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Unigram Frequencies Before Handling Unknown Words:\n",
      "('hotel',): 1146\n",
      "('room',): 1140\n",
      "('stay',): 417\n",
      "('great',): 356\n",
      "('n',): 339\n",
      "('chicago',): 329\n",
      "('would',): 327\n",
      "('night',): 294\n",
      "('staff',): 270\n",
      "('service',): 266\n",
      "\n",
      "Top 10 Bigram Frequencies Before Handling Unknown Words:\n",
      "('front', 'desk'): 103\n",
      "('room', 'service'): 66\n",
      "('hotel', 'room'): 43\n",
      "('michigan', 'ave'): 36\n",
      "('stay', 'hotel'): 36\n",
      "('could', 'n'): 33\n",
      "('bed', 'comfortable'): 32\n",
      "('recommend', 'hotel'): 32\n",
      "('great', 'location'): 31\n",
      "('room', 'clean'): 31\n",
      "\n",
      "Top 10 Unigram Probabilities Before Handling Unknown Words:\n",
      "('hotel',): 0.0287\n",
      "('room',): 0.0285\n",
      "('stay',): 0.0104\n",
      "('great',): 0.0089\n",
      "('n',): 0.0085\n",
      "('chicago',): 0.0082\n",
      "('would',): 0.0082\n",
      "('night',): 0.0074\n",
      "('staff',): 0.0068\n",
      "('service',): 0.0067\n",
      "\n",
      "Top 10 Bigram Probabilities Before Handling Unknown Words:\n",
      "('honoring', 'request'): 1.0000\n",
      "('16th', 'floor'): 1.0000\n",
      "('constitute', 'placing'): 1.0000\n",
      "('placing', 'someone'): 1.0000\n",
      "('justify', 'decide'): 1.0000\n",
      "('decide', 'stay'): 1.0000\n",
      "('believed', 'would'): 1.0000\n",
      "('keihl', 'bath'): 1.0000\n",
      "('junior', 'suite'): 1.0000\n",
      "('lawry', 'accross'): 1.0000\n",
      "\n",
      "Top 10 Unigram Probabilities After Handling Unknown Words using BPE:\n",
      "('<UNK>',): 0.1587\n",
      "('hotel',): 0.0287\n",
      "('room',): 0.0285\n",
      "('stay',): 0.0104\n",
      "('great',): 0.0089\n",
      "('n',): 0.0085\n",
      "('chicago',): 0.0082\n",
      "('would',): 0.0082\n",
      "('night',): 0.0074\n",
      "('staff',): 0.0068\n",
      "\n",
      "Top 10 Bigram Probabilities After Handling Unknown Words using BPE:\n",
      "('junior', 'suite'): 1.0000\n",
      "('health', 'club'): 1.0000\n",
      "('9th', 'floor'): 1.0000\n",
      "('ca', 'n'): 1.0000\n",
      "('centrally', 'located'): 1.0000\n",
      "('continental', 'breakfast'): 1.0000\n",
      "('wi', 'fi'): 1.0000\n",
      "('29th', 'floor'): 1.0000\n",
      "('wo', 'n'): 1.0000\n",
      "('la', '<UNK>'): 1.0000\n",
      "\n",
      "Top 10 Unigram Frequencies After Handling Unknown Words using BPE:\n",
      "('<UNK>',): 6341\n",
      "('hotel',): 1146\n",
      "('room',): 1140\n",
      "('stay',): 417\n",
      "('great',): 356\n",
      "('n',): 339\n",
      "('chicago',): 329\n",
      "('would',): 327\n",
      "('night',): 294\n",
      "('staff',): 270\n",
      "\n",
      "Top 10 Bigram Frequencies After Handling Unknown Words using BPE:\n",
      "('<UNK>', '<UNK>'): 1488\n",
      "('<UNK>', 'room'): 173\n",
      "('hotel', '<UNK>'): 159\n",
      "('room', '<UNK>'): 155\n",
      "('<UNK>', 'hotel'): 136\n",
      "('front', 'desk'): 103\n",
      "('room', 'service'): 66\n",
      "('<UNK>', 'floor'): 56\n",
      "('service', '<UNK>'): 54\n",
      "('<UNK>', 'would'): 50\n",
      "\n",
      "Perplexity of BPE bigram model with Laplace smoothing: 595.9665\n",
      "\n",
      "Perplexity of BPE bigram model with Add-k smoothing: 468.4038\n",
      "\n",
      "Top 10 Unigram Probabilities After Handling Unknown Words using Nearest Neighbor:\n",
      "('hotel',): 0.0287\n",
      "('room',): 0.0285\n",
      "('stay',): 0.0104\n",
      "('great',): 0.0089\n",
      "('n',): 0.0085\n",
      "('chicago',): 0.0082\n",
      "('would',): 0.0082\n",
      "('night',): 0.0074\n",
      "('staff',): 0.0068\n",
      "('service',): 0.0067\n",
      "\n",
      "Top 10 Bigram Probabilities After Handling Unknown Words using Nearest Neighbor:\n",
      "('honoring', 'request'): 1.0000\n",
      "('16th', 'floor'): 1.0000\n",
      "('constitute', 'placing'): 1.0000\n",
      "('placing', 'someone'): 1.0000\n",
      "('justify', 'decide'): 1.0000\n",
      "('decide', 'stay'): 1.0000\n",
      "('believed', 'would'): 1.0000\n",
      "('keihl', 'bath'): 1.0000\n",
      "('junior', 'suite'): 1.0000\n",
      "('lawry', 'accross'): 1.0000\n",
      "\n",
      "Top 10 Unigram Frequencies After Handling Unknown Words using Nearest Neighbor:\n",
      "('hotel',): 1146\n",
      "('room',): 1140\n",
      "('stay',): 417\n",
      "('great',): 356\n",
      "('n',): 339\n",
      "('chicago',): 329\n",
      "('would',): 327\n",
      "('night',): 294\n",
      "('staff',): 270\n",
      "('service',): 266\n",
      "\n",
      "Top 10 Bigram Frequencies After Handling Unknown Words using Nearest Neighbor:\n",
      "('front', 'desk'): 103\n",
      "('room', 'service'): 66\n",
      "('hotel', 'room'): 43\n",
      "('michigan', 'ave'): 36\n",
      "('stay', 'hotel'): 36\n",
      "('could', 'n'): 33\n",
      "('bed', 'comfortable'): 32\n",
      "('recommend', 'hotel'): 32\n",
      "('great', 'location'): 31\n",
      "('room', 'clean'): 31\n",
      "\n",
      "Perplexity of Nearest Neighbor bigram model with Laplace smoothing: 2240329.9970\n",
      "\n",
      "Perplexity of Nearest Neighbor bigram model with Add-k smoothing: 1897492.6901\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    # Load stopwords and lemmatizer\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Load and preprocess dataset\n",
    "    train_file = \"C:/Users/ruthw/Desktop/Uni/NPL/A1_DATASET/A1_DATASET/train.txt\"\n",
    "    validation_file = \"C:/Users/ruthw/Desktop/Uni/NPL/A1_DATASET/A1_DATASET/val.txt\"\n",
    "    \n",
    "    train_tokens = read_corpus(train_file, stop_words, lemmatizer)\n",
    "    validation_tokens = read_corpus(validation_file, stop_words, lemmatizer)\n",
    "    \n",
    "    # Train Word2Vec model\n",
    "    word2vec_model = Word2Vec([train_tokens], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "    # Display top unigram and bigram frequencies before handling unknown words\n",
    "    unigram_counts_before = count_ngrams(train_tokens, 1)\n",
    "    bigram_counts_before = count_ngrams(train_tokens, 2)\n",
    "\n",
    "    unigram_freqs_before = Counter(unigram_counts_before).most_common(10)\n",
    "    bigram_freqs_before = Counter(bigram_counts_before).most_common(10)\n",
    "\n",
    "    print(\"\\nTop 10 Unigram Frequencies Before Handling Unknown Words:\")\n",
    "    for unigram, count in unigram_freqs_before:\n",
    "        print(f\"{unigram}: {count}\")\n",
    "\n",
    "    print(\"\\nTop 10 Bigram Frequencies Before Handling Unknown Words:\")\n",
    "    for bigram, count in bigram_freqs_before:\n",
    "        print(f\"{bigram}: {count}\")\n",
    "\n",
    "    # Calculate and print unsmoothed probabilities for unigrams and bigrams before handling unknowns\n",
    "    unigram_probs_before = calculate_probabilities(unigram_counts_before)\n",
    "    bigram_probs_before = calculate_probabilities(bigram_counts_before, unigram_counts_before)\n",
    "\n",
    "    print(\"\\nTop 10 Unigram Probabilities Before Handling Unknown Words:\")\n",
    "    for unigram, prob in Counter(unigram_probs_before).most_common(10):\n",
    "        print(f\"{unigram}: {prob:.4f}\")\n",
    "\n",
    "    print(\"\\nTop 10 Bigram Probabilities Before Handling Unknown Words:\")\n",
    "    for bigram, prob in Counter(bigram_probs_before).most_common(10):\n",
    "        print(f\"{bigram}: {prob:.4f}\")\n",
    "\n",
    "    # Handle unknown words using Byte Pair Encoding (BPE)\n",
    "    train_tokens_bpe = handle_unknown_words_bpe(train_tokens)\n",
    "    validation_tokens_bpe = handle_unknown_words_bpe(validation_tokens)\n",
    "    \n",
    "    # Handle unknown words using Nearest Neighbor Replacement\n",
    "    train_tokens_nn = handle_unknown_words_nearest_neighbor(train_tokens, word2vec_model)\n",
    "    validation_tokens_nn = handle_unknown_words_nearest_neighbor(validation_tokens, word2vec_model)\n",
    "    \n",
    "    # Loop through different unknown handling methods and smoothing techniques\n",
    "    unknown_handling_methods = [\"BPE\", \"Nearest Neighbor\"]\n",
    "    smoothing_methods = [\"Laplace\", \"Add-k\"]\n",
    "\n",
    "    for method in unknown_handling_methods:\n",
    "        if method == \"BPE\":\n",
    "            train_tokens_method = train_tokens_bpe\n",
    "            validation_tokens_method = validation_tokens_bpe\n",
    "        elif method == \"Nearest Neighbor\":\n",
    "            train_tokens_method = train_tokens_nn\n",
    "            validation_tokens_method = validation_tokens_nn\n",
    "\n",
    "        # Unsmoothed n-gram counts\n",
    "        unigram_counts_method = count_ngrams(train_tokens_method, 1)\n",
    "        bigram_counts_method = count_ngrams(train_tokens_method, 2)\n",
    "\n",
    "        # Calculate and print unsmoothed probabilities after handling unknown words\n",
    "        unigram_probs_method = calculate_probabilities(unigram_counts_method)\n",
    "        bigram_probs_method = calculate_probabilities(bigram_counts_method, unigram_counts_method)\n",
    "\n",
    "        print(f\"\\nTop 10 Unigram Probabilities After Handling Unknown Words using {method}:\")\n",
    "        for unigram, prob in Counter(unigram_probs_method).most_common(10):\n",
    "            print(f\"{unigram}: {prob:.4f}\")\n",
    "\n",
    "        print(f\"\\nTop 10 Bigram Probabilities After Handling Unknown Words using {method}:\")\n",
    "        for bigram, prob in Counter(bigram_probs_method).most_common(10):\n",
    "            print(f\"{bigram}: {prob:.4f}\")\n",
    "\n",
    "        # Display top unigram and bigram frequencies after handling unknown words\n",
    "        unigram_freqs_method = Counter(unigram_counts_method).most_common(10)\n",
    "        bigram_freqs_method = Counter(bigram_counts_method).most_common(10)\n",
    "\n",
    "        print(f\"\\nTop 10 Unigram Frequencies After Handling Unknown Words using {method}:\")\n",
    "        for unigram, count in unigram_freqs_method:\n",
    "            print(f\"{unigram}: {count}\")\n",
    "\n",
    "        print(f\"\\nTop 10 Bigram Frequencies After Handling Unknown Words using {method}:\")\n",
    "        for bigram, count in bigram_freqs_method:\n",
    "            print(f\"{bigram}: {count}\")\n",
    "\n",
    "        # Loop through smoothing methods\n",
    "        for smoothing in smoothing_methods:\n",
    "            if smoothing == \"Laplace\":\n",
    "                bigram_probs_smoothed = apply_laplace_smoothing(bigram_counts_method, unigram_counts_method, len(unigram_counts_method), 2)\n",
    "            elif smoothing == \"Add-k\":\n",
    "                bigram_probs_smoothed = apply_add_k_smoothing(bigram_counts_method, unigram_counts_method, len(unigram_counts_method), 2, k=0.5)\n",
    "\n",
    "            # Calculate perplexity\n",
    "            perplexity = calculate_perplexity(validation_tokens_method, bigram_probs_smoothed, 2)\n",
    "            print(f\"\\nPerplexity of {method} bigram model with {smoothing} smoothing: {perplexity:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
